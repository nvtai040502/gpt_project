[
  {
    "objectID": "nano.html",
    "href": "nano.html",
    "title": "Nano GPT",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\n\n# Hyperparameters\nblock_size = 256\nn_head = 6\nn_emb = 384\nbatch_size = 64\nlr = 6e-4\nn_epochs = 5000\nn_layers = 8\n\n\nwith open('/content/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n# Embedding\nchars = sorted(set(text))\nvocab_size = len(chars)\n\n# Character Encoding\nstoi = {s:i for i, s in enumerate(chars)}\nitos = {i:s for s, i in stoi.items()}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l:  ''.join([itos[i] for i in l])\n\n# Convert all text data to integers\ndata = torch.tensor(encode(text), dtype = torch.long)\n\n\ndef get_batch(data, batch_size, block_size, device):\n    # Generate random indices within the valid range\n    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n    # Extract blocks of data using the generated indices\n    xb = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Extract corresponding target blocks\n    # Note that yb is reshaped to a 1D tensor\n    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n\n    return xb.to(device), yb.to(device)\n\nxb, yb = get_batch(data, 64, 8, device)\nxb.shape, yb.shape\n\n(torch.Size([64, 8]), torch.Size([512]))\n\n\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, n_emb, block_size, head_size, device):\n        super().__init__()\n\n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n\n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size)).to(device)\n\n    def forward(self, x):\n        B,T,C = x.shape\n\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n\n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n\n        # Masking to make sure the network can't attend to the future positions\n        wei.masked_fill_(self.tril[:T, :T] == 0, float('-inf'))\n\n        # Applying softmax to get the attention probabilities\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_emb, n_head, head_size, device):\n        super().__init__()\n        self.head_size = head_size\n\n        self.heads = nn.ModuleList([Head(n_emb, block_size, head_size, device) for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n\n        # Project the concatenated results\n        out = self.proj(out)\n\n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, n_emb, n_head, device):\n        super().__init__()\n\n        head_size = n_emb // n_head\n\n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_emb, n_head, head_size, device)\n\n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n\n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n\n    def forward(self, x):\n        # Multi-Head Attention Block\n        x = x + self.mul_head(self.ln1(x))\n\n        # Feed-Forward Block\n        x = x + self.ffwd(self.ln2(x))\n\n        return x\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers, device):\n        super().__init__()\n        self.n_layers = n_layers\n\n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n\n        # Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_emb, n_head, device) for _ in range(n_layers)])\n\n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n\n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n\n    def forward(self, inp, targets = None):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.shape[1], device = device))\n        x_emb = token_emb + position_emb\n\n        # Transformer blocks\n        out = self.blocks(x_emb)\n\n        # Final layer normalization\n        out = self.ln_f(out)\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n\n        if targets == None:\n          loss = None\n        else:\n          # Reshape logits for the cross-entropy loss\n          logits = logits.view(-1, logits.shape[-1])\n          # Compute the cross-entropy loss\n          loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n\nmodel = BigramLanguageModel(vocab_size, 32, 8, 5, 1, device)\nmodel.to(device)\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 14817\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n\nfor i in range(100):\n    xb, yb = get_batch(data, 64, 8, device)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(3.2507, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\ncontext = torch.zeros(1, 1, dtype = torch.long, device = device)\ncontext\n\ntensor([[0]], device='cuda:0')\n\n\n\nlogits, _ = model(context)\nlogits, logits.shape\n\n(tensor([[[ 1.4532,  2.6264, -0.8126, -1.4322, -2.1681, -0.8644,  0.4027,\n           -1.0778, -1.0430, -1.5876, -0.7976, -0.3464, -1.5724, -0.4819,\n           -0.4304, -1.0133, -1.5605, -0.8962, -1.4719, -1.2092, -0.6767,\n           -0.7557, -2.4317, -0.9886, -1.3998, -0.8497, -1.0715, -0.5712,\n           -1.4491, -1.8244, -1.1285, -0.6197, -0.4073, -1.3509, -1.4581,\n           -1.0151, -0.5341, -1.5363, -2.2366,  1.2976,  0.1806,  0.0059,\n            0.4877,  1.7831,  0.3746, -0.1175,  0.9814,  1.0195, -2.1851,\n           -0.5097,  0.4699,  0.6254,  0.6517,  1.4879, -0.5892, -1.6162,\n            0.4139,  0.7043,  1.0963,  0.6129, -1.0932,  0.2736, -1.5890,\n            0.4178, -1.9275]]], device='cuda:0', grad_fn=&lt;ViewBackward0&gt;),\n torch.Size([1, 1, 65]))\n\n\n\nlogits = logits[:, -1, :]\nlogits.shape\n\ntorch.Size([1, 65])\n\n\n\nprobs = F.softmax(logits, dim=-1)\nprobs\n\ntensor([[0.0556, 0.1796, 0.0058, 0.0031, 0.0015, 0.0055, 0.0194, 0.0044, 0.0046,\n         0.0027, 0.0059, 0.0092, 0.0027, 0.0080, 0.0084, 0.0047, 0.0027, 0.0053,\n         0.0030, 0.0039, 0.0066, 0.0061, 0.0011, 0.0048, 0.0032, 0.0056, 0.0045,\n         0.0073, 0.0031, 0.0021, 0.0042, 0.0070, 0.0086, 0.0034, 0.0030, 0.0047,\n         0.0076, 0.0028, 0.0014, 0.0476, 0.0156, 0.0131, 0.0212, 0.0773, 0.0189,\n         0.0116, 0.0347, 0.0360, 0.0015, 0.0078, 0.0208, 0.0243, 0.0249, 0.0575,\n         0.0072, 0.0026, 0.0197, 0.0263, 0.0389, 0.0240, 0.0044, 0.0171, 0.0027,\n         0.0197, 0.0019]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nidx_next = torch.multinomial(probs, num_samples=1)\nidx_next\n\ntensor([[43]], device='cuda:0')\n\n\n\ncontext = torch.cat((context, idx_next), dim=1) # (B, T+1)\ncontext\n\ntensor([[ 0, 43]], device='cuda:0')\n\n\n\ncontext = torch.zeros(1, 1, dtype = torch.long, device = device)\nfor i in range(10):\n  idx_cond = context[:, -8:]\n  logits, _ = model(idx_cond.to(device))\n  logits = logits[:, -1, :]\n  probs = F.softmax(logits, dim=-1)\n  idx_next = torch.multinomial(probs.to(device), num_samples=1)\n  context = torch.cat((context, idx_next), dim=1)\ncontext\n\ntensor([[ 0,  1, 42,  1, 54, 46, 56, 41, 46, 63, 59]], device='cuda:0')\n\n\n\nprint(decode(context[0].tolist()))\n\n\n d phrchyu\n\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers, device):\n        super().__init__()\n        self.n_layers = n_layers\n        self.block_size = block_size\n\n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n\n        # Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_emb, n_head, device) for _ in range(n_layers)])\n\n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n\n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n\n    def forward(self, inp, targets = None):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.shape[1], device = device))\n        x_emb = token_emb + position_emb\n\n        # Transformer blocks\n        out = self.blocks(x_emb)\n\n        # Final layer normalization\n        out = self.ln_f(out)\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n\n        if targets == None:\n          loss = None\n        else:\n          # Reshape logits for the cross-entropy loss\n          logits = logits.view(-1, logits.shape[-1])\n          # Compute the cross-entropy loss\n          loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\n\nmodel = BigramLanguageModel(vocab_size, 32, 8, 5, 1, device)\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n\nfor i in range(100):\n    xb, yb = get_batch(data, 64, 8, device)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\ncontext = torch.zeros(1, 1, dtype = torch.long, device = device)\ncontext = model.generate(context, 100)\nprint(decode(context[0].tolist()))\n\n\nlAiangego qt s rhrditts yGED oEM thloXtoYbe&tnsaweco&ilx e,zv: ostnGho  tris'Tste elbcolrPguQ  K&d b\n\n\n\nimport time\n# Get the start time\nstart_time = time.time()\n\nmodel = BigramLanguageModel(vocab_size, batch_size, block_size, n_head, n_layers, device)\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size, device)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n# Get the end time\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\n\n# Convert to minutes and seconds\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2)\n\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\nprint(f\"Loss: {loss}\")\n\nModel training time: 12 min 32.97s\nLoss: 1.3645884990692139\n\n\n\ncontext = torch.zeros(1, 1, dtype = torch.long, device=device)\ncontext = model.generate(context, 5000)\nprint(decode(context[0].tolist()))\n\n\n\nBINCHURD:\nHow would prosom.\n\nCORIOLANUS:\nWithre, wish'd Tyrrel! What! sovereign will their wound\nThe leason, mean must bone of any wipsoy,\nSprenators are the unclaol you both:\nI did! Lord not calmise she himself,\nSo not time and sto let your griop,\nAnd not themself he; but fond Edward's;\nI dothous of my this nection Ledia cravol'd thousand:\nOr thou murderst myself by moron.\nHere good no fheash which, for his means: not\nThat may massy pass himself.\n\nSecondamen:\nWhen follows of the ragin of at Juldeen.\n\nRICHMOND:\nArtimence\nO had him; he down, if you'rt,--that'd hearts; where's young,\nHe is on, the my life.\n\nBids Senator:\nYet be do no tongue, which you manuch London.\nIf you, myself, I'll swear'd life de'er anger,\nfor you should leave you, beholds on dawn;\nIf you my widest throat dost your did.\n\nSICINIUS:\nDurstily lamish their fly pirdon, vourtur you\nOr herume, your upon to you with's.\n\nJOMN.\n\nCALISA:\nAnd mayling her And God, and live.\n\nBoold Marcius of Glord!\n\nTRANIO:\nI have home; and, I neven been her with easl.\n\nNuse:\nNo, if you comble know the were moonst thou state,\nLet in Rutless, I will for an my compage,\nWere foom out now sault.\n\nLUCIO:\nAlisa! Ha, vife, somemans, only thy earth,\nThis rancions Isap live; on her Volsce,\nA famought menalsy all you thine men his,\nBut now the could give of Kentom where's thou none:\nPadam, help My hath should death in away't.\n\nDUKE OF YORK:\nBaptatiOSons, thou yourself: how tidstruled are\nThe beet that coasks that him: I will flesh,\nStill thou warried, I am benefit's band,\nWhy having end he's tongue a shown. My lord;\nAy, to veing, he must will nicemploidy:\nWhy, brother? stays the your nust chiefs like half?\nfor the lord, gay you greating to its is mine;\nHow sorrow to-taffe, sir blood him.\n\nGANE:\nNo, whilst thou vinechably deep mean\nShould he denilsh. I humbout him, poort them well.\nYes no loyalong a mother, doth daughter-scrillence\nRun thou menfords: my doom Edward's pieces\nwith neverfules are, and like no.\nI thene--is I, away good bittle him time\nApprouched not the wears earth come his out:\nI'll peach\nWhich worse this dafby after tearth:\nAnd no esport yet through Stiffortly:\nFound by mostion, and seems yearsen,\nTo him blood betwixt with right forbeign.\nAvoiding Gentleman,\nUpon of you suith'd from.\nTake I not your maje, the thought sleep from Brancish her.\n\nVALENIO:\nAnd not proominal; flear, I'll your may.\n\nBENVOLIO:\nHow moust that donature heart, by strept's not\nseek the one power ashe crevent out.\n\nSecond Citizen:\nThat thy could lords, O meet them name,\nThe show will powery shall thee bashedly wretch,\nThat here a mile do.\n\nCAPULET:\nGo anst madam, Sir, that you comes wont pilt.\nWhen the poor acheect in the charm.\n\nESCALUS:\nThe world here quarrelly, my father's well.\n\nPETRUCHIO:\nWith so. Why he had Six Aumey?\n\nLOMETEL:\nNay, so marry neather me whom hie be fearful so.\n\nGLOUCESTER:\nLet shall a splew'd util raise Mauch done.\nKing Vistiber in false him improcy!\n\nNORTHUMBERLAND:\nGood Retranged, our my long; as prison,--\n\nLADY ANNE:\nI must don, madne when thou stay the temw Edward?\n\nDUCHESS OF YORK:\nKnow night-scaley's make addening,--Those she spiliented.\nThe Chatard host known thy bound came;\nBith thou sand now slain are, leave not the\nTime friar a caulamony strongerous; when my peace.\nWhat? have are it my vial man\nDeceince scopul'd but not this do\nUnheav'd look brother: we heaven! Guession.\n\nNurse:\nI need.\n\nELWARD:\nAy, say, if impatance this childrent him;\nAs he wrong a made poising modes and most\nto fact not pleasure of the dispak, if all,\nYour hauntily come; my Lnord As stay the is,\nThat by jollior at bearn, -my wife;\nThou shouldst corn in he'r beaut, the word.\n\nDUKE OF YORK:\nHow are now, there, them, her: And, forbund then\nObacter's dattly nothing restors: he daughter.\n\nCORIOLANUS:\nWile she sister, as wish birmly down.\n\nDUKE VINCENTIO:\nThat he good much, thou worst may thou accuse,\nAnd how herself cares change art to\nmight now! Captaius\n\nGLOUCESTER:\nI would a good ap, I have redly Georg the so beggar,\nThou knew'st not to have much well to noble;\nAnd auntiver PomuLe Edward's not me-both;\nI'll say repingry,\nTybalt, my lordnex Duked.\n\nPETRUCHIO:\nIn have won his tormish'd provope, an Canstator!\nTherefort, downs by Grumishanes, west I execute\nThat anscher-bone fearful encouse\n'Than so with your sixtemneds. Who silves,\nThe grien every had left of his affeir,\nCousin, no mishact I tate a sail wint.\nI brave whom sword thou had thou sea\nTo wash on the murder'st rest of andswer.\n\nARIEL:\nTherefore, if left your unded.\n\nJULIET:\nLaster, in young benotators of the old:\nAid by my heart grace wholow him excute's\nAs idlections by to our lifess constrial\nOf theirly prepose that that early a\nFoncy's of countrensment to us, but you inclove.\nAn should the world.\n\nKING RICHARD III:\nIt prisons feellutes, hath man incladed\nThe tongle, the Romey's liuse his in\nAridey of an glife bonef, insweet him men.\nThere hunger spiake tone so hearl--\nConductioush. Whose my he tyble heavens and possak.\nWhat a wonte"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gpt_project",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "gpt_project",
    "section": "Install",
    "text": "Install\npip install gpt_project"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "gpt_project",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "knowledge.html",
    "href": "knowledge.html",
    "title": "Attention Architecture",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "knowledge.html#the-goal",
    "href": "knowledge.html#the-goal",
    "title": "Attention Architecture",
    "section": "The Goal",
    "text": "The Goal\nTrong chương này, mục tiêu hàng đầu của chúng ta là khám phá một cách chi tiết và cụ thể từng bước của quá trình giải mã (decoder) (phần được khoanh đỏ) dựa trên kiến trúc Attention, như hình minh họa dưới đây:\n\nChúng ta sẽ đảm bảo rằng mỗi bước trong quy trình này được diễn giải một cách chi tiết để chúng ta có thể hiểu sâu hơn về cách nó hoạt động và tương tác với dữ liệu đầu vào. Điều quan trọng là thông qua việc làm này, chúng ta sẽ có cơ hội thấu hiểu rõ hơn về cách áp dụng kiến thức này vào các dự án thực tế, ví dụ như xây dựng một Mô hình Ngôn ngữ Lớn (Large Language Model) cho tiếng Việt, mở ra nhiều tiềm năng ứng dụng hấp dẫn."
  },
  {
    "objectID": "knowledge.html#tiny-shakespeare",
    "href": "knowledge.html#tiny-shakespeare",
    "title": "Attention Architecture",
    "section": "Tiny Shakespeare",
    "text": "Tiny Shakespeare\n\nwith open('data/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(type(text))    \nprint(\"length of dataset in characters: \", len(text))\n\n&lt;class 'str'&gt;\nlength of dataset in characters:  1115394\n\n\nTập dữ liệu “tiny Shakespeare” là một kho văn bản chứa các tác phẩm của danh tác William Shakespeare, với hơn 1 triệu ký tự. Mục tiêu chính của việc sử dụng tập dữ liệu này là xây dựng một mô hình mạng neural có khả năng dự đoán ký tự tiếp theo trong một đoạn văn dựa trên các ký tự trước đó. Mô hình này sẽ có khả năng tái tạo cấu trúc và phong cách viết của Shakespeare, tạo ra văn bản một cách tự nhiên và đầy hấp dẫn.\n\n# let's look at the first 200 characters\nprint(text[:200])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you"
  },
  {
    "objectID": "knowledge.html#process-data",
    "href": "knowledge.html#process-data",
    "title": "Attention Architecture",
    "section": "Process Data",
    "text": "Process Data\n\n# Embedding\nchars = sorted(set(text))\nprint(''.join(chars))\n\nvocab_size = len(chars)\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\nTrong dự án này, chúng ta sẽ sử dụng một phương pháp nhúng (embedding) đơn giản. Cụ thể, chúng ta sẽ xác định tất cả các ký tự duy nhất có trong toàn bộ tập dữ liệu và gán một số duy nhất cho mỗi ký tự này.\nTrong tập dữ liệu của chúng ta, có tổng cộng 65 ký tự khác nhau. Mục tiêu chính của dự án là xây dựng một mô hình có khả năng dự đoán ký tự tiếp theo nằm trong 65 ký tự này. Điều này có nghĩa rằng chúng ta muốn mô hình học cách dự đoán ký tự tiếp theo dựa trên ngữ cảnh và phân tích các ký tự trước đó trong chuỗi văn bản.\n\n# Character Encoding\nstoi = {s:i for i, s in enumerate(chars)}\nitos = {i:s for s, i in stoi.items()}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l:  ''.join([itos[i] for i in l])\n\nỞ đây, chúng ta đang thực hiện một quá trình được gọi là “mã hóa ký tự (character encoding)”. Trong quá trình này, mỗi ký tự riêng biệt trong dữ liệu của chúng ta sẽ được ánh xạ thành một số nguyên tương ứng. Chúng ta thực hiện việc này để có khả năng chuyển đổi linh hoạt giữa chuỗi ký tự và số nguyên, giúp chúng ta hiểu và biểu diễn kết quả một cách dễ dàng và hiệu quả hơn.\n\ntext_exp = \"Hello World\"\nprint(encode(text_exp))\nprint(decode(encode(text_exp)))\n\n[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\nHello World\n\n\n\n# Convert all text data to integers\ndata = torch.tensor(encode(text), dtype = torch.long)\ndata[:16]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14])\n\n\nChuyển mọi ký tự trong dữ liệu “Tiny Shakespeare” sang số nguyên.\n\n# Define block size and batch size\nblock_size = 8\nbatch_size = 4\n\n# Generate random indices within the valid range\nix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n# Extract blocks of data using the generated indices\nxb = torch.stack([data[i:i+block_size] for i in ix])\n\n# Extract corresponding target blocks\n# Note that yb is reshaped to a 1D tensor\nyb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n\n# Print the shapes of xb and yb\nprint(\"xb.shape:\", xb.shape)\nprint(\"yb.shape:\", yb.shape)\n\nxb.shape: torch.Size([4, 8])\nyb.shape: torch.Size([32])\n\n\nBên trên là một ví dụ minh họa về cách tạo x_batch và y_batch sử dụng batch_size, trong đó x_batch được cố định theo block size.\n\nfor i in range(block_size):\n    inp = xb[0, :i+1].tolist()\n    target = yb[i]\n    print(f\"Input: {inp} --&gt; Target: {target}\")\n\nInput: [58] --&gt; Target: 46\nInput: [58, 46] --&gt; Target: 39\nInput: [58, 46, 39] --&gt; Target: 58\nInput: [58, 46, 39, 58] --&gt; Target: 1\nInput: [58, 46, 39, 58, 1] --&gt; Target: 58\nInput: [58, 46, 39, 58, 1, 58] --&gt; Target: 46\nInput: [58, 46, 39, 58, 1, 58, 46] --&gt; Target: 43\nInput: [58, 46, 39, 58, 1, 58, 46, 43] --&gt; Target: 56"
  },
  {
    "objectID": "knowledge.html#hyperparameters",
    "href": "knowledge.html#hyperparameters",
    "title": "Attention Architecture",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nlearning_rate = 1e-3\nn_epochs = 1500\n\nvocab_size = len(chars)\nn_emb = 32\n\nbatch_size = 64\nblock_size = 8\n\nhead_size = 20\nn_head = 4\n\nĐể tránh sự lặp lại không cần thiết trong quá trình giải thích cách thực hiện, chúng ta sẽ duy trì liên tục các biến sau đây."
  },
  {
    "objectID": "knowledge.html#distributed-presentation",
    "href": "knowledge.html#distributed-presentation",
    "title": "Attention Architecture",
    "section": "Distributed Presentation",
    "text": "Distributed Presentation\n\n# Define the embedding size\nC = torch.randn(vocab_size, n_emb)\n\nweight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\nbias = torch.zeros(vocab_size)\n\nparameters = [C, weight, bias]\n\nnum_parameters = 0\nfor p in parameters:\n    p.requires_grad = True\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4225\n\n\n\n# Forward sample\nx_emb = C[xb]  # Embedding lookup for input data\nprint(\"x_emb.shape:\", x_emb.shape)\n\n# Compute logits using a linear transformation\nlogits = x_emb @ weight + bias\nprint(\"logits.shape:\", logits.shape)\n\n# Reshape logits for the cross-entropy loss\nlogits = logits.view(-1, logits.shape[-1])\n\n# Compute the cross-entropy loss\nloss = F.cross_entropy(logits, yb)\nprint(\"loss:\", loss)\n\nx_emb.shape: torch.Size([4, 8, 32])\nlogits.shape: torch.Size([4, 8, 65])\nloss: tensor(4.7028, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\noptimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n\nfor epochi in range(n_epochs):\n    # Generate random indices within the valid range\n    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n    # Extract blocks of data using the generated indices\n    xb = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Extract corresponding target blocks\n    # Note that yb is reshaped to a 1D tensor\n    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n\n    x_emb = C[xb]  # Embedding lookup for input data\n\n    # Compute logits using a linear transformation\n    logits = x_emb @ weight + bias\n\n    # Reshape logits for the cross-entropy loss\n    logits = logits.view(-1, logits.shape[-1])\n\n    # Compute the cross-entropy loss\n    loss = F.cross_entropy(logits, yb)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.4565, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "knowledge.html#attention-mechanism",
    "href": "knowledge.html#attention-mechanism",
    "title": "Attention Architecture",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\n1. Position\n\nC = torch.randn(vocab_size, n_emb)\n\nweight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\nbias = torch.zeros(vocab_size)\n\n# New code\n# ------------------------------------------------------------\nposition = torch.randn(block_size, n_emb) * block_size **-0.5\nparameters = [C, weight, bias, position]\n# ------------------------------------------------------------\n\nnum_parameters = 0\nfor p in parameters:\n    p.requires_grad = True\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4481\n\n\nTrong phương pháp “Distributed Presentation,” ban đầu chúng ta đã sử dụng việc nhúng (embedding) để ánh xạ từng từ vào một vector đặc trưng riêng biệt (matrix C). Tuy nhiên, để nâng cao khả năng biểu diễn, chúng ta muốn không chỉ biết về vector đặc trưng (C) của ký tự mà còn quan tâm đến vị trí (position) của ký tự đó trong câu.\nĐể thực hiện điều này, chúng ta sẽ tạo ra một ma trận vị trí mới (matrix position). Trong ma trận này, mỗi hàng sẽ tương ứng với một vị trí trong câu và nó sẽ được sử dụng để kết hợp với vector đặc trưng (C) của ký tự tại vị trí tương ứng, giúp cải thiện khả năng biểu diễn của mô hình.\n\ndef get_batch(data, batch_size, block_size):\n    # Generate random indices within the valid range\n    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n    # Extract blocks of data using the generated indices\n    xb = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Extract corresponding target blocks\n    # Note that yb is reshaped to a 1D tensor\n    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n    \n    return xb, yb\n\nxb, yb = get_batch(data, batch_size, block_size)\nxb.shape, yb.shape\n\n(torch.Size([64, 8]), torch.Size([512]))\n\n\nĐể tránh việc lặp lại mã code và để tạo sự tiện lợi, tôi tạo một hàm có tên là get_batch để tự động tạo các batch x và y dựa trên kích thước batch_size và block_size.\n\noptimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    # New code\n    # -----------------------------------------------\n    xb, yb = get_batch(data, batch_size, block_size)\n    \n    # Embedding lookup for input data\n    x_emb = C[xb]  \n    x_emb += position\n    # -----------------------------------------------\n    \n    # Compute logits using a linear transformation\n    logits = x_emb @ weight + bias\n\n    # Reshape logits for the cross-entropy loss\n    logits = logits.view(-1, logits.shape[-1])\n\n    # Compute the cross-entropy loss\n    loss = F.cross_entropy(logits, yb)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.4990, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n2. Weight Average\nHiện tại, mô hình chỉ dựa vào ký tự và vị trí gần nhất của ký tự đó để thực hiện dự đoán. Tuy nhiên, điều này không đủ hiệu quả. Chúng ta muốn mô hình có khả năng sử dụng tất cả thông tin từ các ký tự trước đó để cải thiện dự đoán ký tự tiếp theo.\nHãy xem xét ví dụ từ hai từ “his” và “like.” Giả sử chúng ta cung cấp cho mô hình các vector đặc trưng biểu diễn cho từ “i” và vị trí thứ hai của từ “i” trong câu. Tuy nhiên, trong trường hợp này, mô hình có thể dự đoán ký tự tiếp theo là “s” hoặc “k” mà không có thông tin đủ để quyết định. Điều quan trọng là chúng ta cần cung cấp cho mô hình thông tin về ký tự “h” đứng trước ký tự “i” thay vì “l” để mô hình có thể học được và dự đoán đúng ký tự “s” là ký tự tiếp theo.\n\n# Lower triangular matrix for masking\ntril = torch.tril(torch.ones(block_size, block_size))\n\n# Masking to make sure the network can't attend to the future positions\nwei = tril.masked_fill(tril==0, float('-inf'))\n\n# Applying softmax to get the attention probabilities\nwei = F.softmax(wei, dim=-1)\nprint(wei)\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nDo đó, chiến lược tạm thời của chúng ta ở đây là tích hợp thông tin từ tất cả các ký tự trước đó, đã xuất hiện, bằng cách tính trung bình của các vector biểu diễn và vị trí của chúng. Điều này sẽ giúp mô hình dự đoán từ tiếp theo một cách chính xác hơn.\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n\nx_emb = C[xb]\nx_emb +=  position\n\n# New code\n# -------------------------------------------------------\n# Lower triangular matrix for masking\ntril = torch.tril(torch.ones(block_size, block_size))\n\n# Masking to make sure the network can't attend to the future positions\nwei = tril.masked_fill(tril==0, float('-inf'))\n\n# Applying softmax to get the attention probabilities\nwei = F.softmax(wei, dim=-1)\n\nout = wei @ x_emb\n# --------------------------------------------------------\n\n# Print the first 3 elements of the original input and the transformed input for the first block\nprint(f\"Original Input (first batch, first 3 characters):\\n {x_emb[0, :3]}\")\nprint(\"\")\nprint(f\"Transformed Input (first batch, first 3 characters):\\n {out[0, :3]}\")\n\nOriginal Input (first batch, first 3 characters):\n tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n        [-1.0184, -2.0927, -0.3102,  3.6519, -2.4072,  0.0139,  0.9342, -1.7704,\n         -0.1125, -0.5072, -1.2626, -1.7496, -1.1825,  0.0487, -0.9318,  0.1392,\n         -2.3752,  1.4060, -1.2250,  1.9381,  0.3784, -1.2098,  0.6793,  0.8746,\n         -0.5673, -3.0030,  1.0940, -1.0829,  0.0083,  2.4880, -0.3996,  2.7292],\n        [ 0.4108,  0.8699, -1.0485, -3.0167,  0.0901, -0.1466, -0.6756,  0.8492,\n          0.2072,  1.3359, -0.2287,  1.1866, -1.0809, -1.1253, -0.8090, -1.1819,\n         -2.4869, -3.1272,  0.2352,  0.4746,  0.2054, -1.5705, -1.3706, -0.6790,\n          0.5017, -2.9568,  0.0115, -0.3515,  1.4428, -1.3596, -0.5216,  0.8348]])\n\nTransformed Input (first batch, first 3 characters):\n tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n        [-0.7260, -0.9220, -0.9808,  3.1329, -0.9744,  0.5530, -0.4166, -1.2486,\n          0.9648,  0.8888, -0.1766, -0.5698,  0.1026, -0.1449, -2.1223, -0.2119,\n         -1.5634,  0.8171, -0.8919,  1.8090, -0.2520, -0.2758,  0.7107,  0.3676,\n         -0.9598, -1.7663,  0.9496, -1.4098, -0.0826,  0.9434, -0.7152,  0.8369],\n        [-0.3471, -0.3247, -1.0034,  1.0830, -0.6196,  0.3198, -0.5029, -0.5493,\n          0.7123,  1.0379, -0.1940,  0.0157, -0.2919, -0.4717, -1.6845, -0.5353,\n         -1.8712, -0.4977, -0.5162,  1.3642, -0.0995, -0.7074,  0.0169,  0.0187,\n         -0.4726, -2.1631,  0.6369, -1.0570,  0.4259,  0.1757, -0.6507,  0.8362]])\n\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Embedding layers\n        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n        \n        # Linear layer for language modeling\n        self.weight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\n        self.bias = torch.zeros(vocab_size)\n        \n        self.parameters = [self.C, self.weight, self.bias, self.position]\n        for p in self.parameters:\n            p.requires_grad = True\n\n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        x_emb = self.C[inp] \n        x_emb += position\n        \n        # New code\n        # -------------------------------------------------------------\n        # Lower triangular matrix for masking\n        tril = torch.tril(torch.ones(block_size, block_size))\n        \n        # Masking to make sure the network can't attend to the future positions\n        wei = tril.masked_fill(tril==0, float('-inf'))\n        \n        # Applying softmax to get the attention probabilities\n        wei = F.softmax(wei, dim=1)\n        \n        out = wei @ x_emb\n        logits = out @ weight + bias\n        # -------------------------------------------------------------\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters:\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4481\n\n\n\noptimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(3.0674, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n3. Key, Query, Value\nMô hình hiện tại của chúng ta vẫn chưa đủ hiệu quả, vì chúng ta cần xem xét xác suất quan trọng của các từ đã xuất hiện trước đó đối với việc dự đoán ký tự tiếp theo. Hãy xem xét ví dụ với các từ thay vì ký tự, vì tôi nghĩ điều này có thể giúp chúng ta hiểu rõ hơn.\nVí dụ, trong câu “He is a boy,” để dự đoán từ “boy,” các từ “he” và “is” sẽ có đóng góp quan trọng hơn so với từ “a” trong quá trình dự đoán. Điều này có nghĩa là mô hình cần hiểu được sự liên kết ngữ cảnh giữa các từ và xác định xem từ nào có ảnh hưởng lớn đến dự đoán của mình.\n\n# Get a batch of data\nxb, yb = get_batch(data, batch_size, block_size)\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n# Embed input data\nx_emb = C[xb]\nx_emb += position\n\n# New code\n# ------------------------------------------------------\n# Initialize key and query matrices\nkey = torch.randn(n_emb, head_size) * n_emb ** -0.5\nquery = torch.randn(n_emb, head_size) * n_emb ** -0.5\n\n# Calculate the key and query values\nk = x_emb @ key\nq = x_emb @ query\n\n# Compute the dot product between queries and keys\nwei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\nprint(wei.shape)\n\ntril = torch.tril(torch.ones(block_size, block_size))\nwei = wei.masked_fill(tril==0, float('-inf')) \n# ------------------------------------------------------\n\nwei=F.softmax(wei, dim=-1)\nprint(wei[0])\n\ntorch.Size([64, 8, 8])\ntensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.0000e+00, 4.5888e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [2.2766e-02, 9.3893e-01, 3.8305e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [4.0015e-03, 8.9893e-01, 3.7121e-05, 9.7027e-02, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.4366e-02, 1.1882e-03, 6.8691e-01, 1.9523e-02, 2.7801e-01, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.7339e-15, 1.4864e-16, 9.9915e-01, 8.5241e-04, 5.7747e-15, 1.8133e-14,\n         0.0000e+00, 0.0000e+00],\n        [3.1721e-01, 1.3526e-06, 3.7937e-09, 2.6322e-06, 1.0903e-10, 1.1801e-01,\n         5.6478e-01, 0.0000e+00],\n        [6.3539e-10, 5.0136e-11, 9.9992e-01, 7.7084e-05, 2.7238e-08, 1.9036e-14,\n         2.8848e-12, 4.0533e-11]])\n\n\n\n# New code\n# ------------------------------------------------------\nvalue = torch.randn(n_emb, head_size) * n_emb ** -0.5\nv = x_emb @ value\n\nout = wei @ v\n# ------------------------------------------------------\n\nout.shape\n\ntorch.Size([64, 8, 20])\n\n\nHãy tưởng tượng rằng bạn là một nhà báo nổi tiếng đang thực hiện một cuộc phỏng vấn với một ngôi sao nổi tiếng, và bạn muốn thu thập thông tin quan trọng từ cuộc trò chuyện đó.\n\nKey có thể coi như danh sách câu hỏi bạn chuẩn bị trước cuộc phỏng vấn. Mỗi câu hỏi là một Key, và mỗi câu hỏi sẽ tập trung vào một khía cạnh cụ thể của cuộc trò chuyện. Ví dụ, một Key có thể là “Bạn đã từng giành giải Oscar chưa?”\nValue là câu trả lời mà ngôi sao đưa ra cho từng câu hỏi. Mỗi câu trả lời chứa thông tin quan trọng về cuộc trò chuyện, và nó sẽ được lưu trữ và sử dụng sau này khi bạn cần nắm bắt thông tin cụ thể từ cuộc phỏng vấn. Chúng ta có thể coi câu trả lời này là “value” của câu hỏi.\nQuery là cách bạn đặt câu hỏi hoặc tìm kiếm thông tin trong cuộc phỏng vấn. Khi bạn muốn biết điều gì đó cụ thể hoặc muốn nắm bắt một thông tin quan trọng từ cuộc trò chuyện, bạn sẽ đặt câu hỏi hoặc tạo một “Query” riêng. Ví dụ, “Giới thiệu về những vai diễn nổi bật nhất của bạn?” có thể là một Query.\n\nKhi bạn đặt một câu hỏi (Query), mô hình sẽ so sánh nó với danh sách các câu hỏi trước đó (Key) và quyết định câu trả lời nào (Value) chứa thông tin phù hợp nhất với câu hỏi của bạn. Điều này giống như việc bạn tập trung vào câu hỏi cụ thể nào trong cuộc trò chuyện để thu thập thông tin bạn cần.\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.weight1 = torch.randn(n_emb, 3 * n_emb) * n_emb ** -0.5\n        self.bias1 = torch.zeros(3 * n_emb)\n        \n        self.weight2 = torch.randn(3 * n_emb, n_emb) * ((3 * n_emb) ** -0.5)\n        self.bias2 = torch.zeros(n_emb)\n\n        self.parameters = [self.weight1, self.bias1, self.weight2, self.bias2]\n        \n    def forward(self, x):\n        x = x @ self.weight1 + self.bias1\n        x = F.relu(x)\n        out = x @ self.weight2 + self.bias2\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Embedding layers\n        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n        # New code\n        # ---------------------------------------------------------------\n        self.proj = torch.randn(head_size, n_emb) * head_size ** -0.5\n        \n        self.key = torch.randn(n_emb, head_size) * n_emb ** -0.5\n        self.query = torch.randn(n_emb, head_size) * n_emb ** -0.5\n        self.value = torch.randn(n_emb, head_size) * n_emb ** -0.5\n\n        self.parameters = [self.C, self.position, self.key, self.query, \\\n                           self.value, self.proj] + self.ffwd.parameters\n        # ---------------------------------------------------------------\n\n        for p in self.parameters:\n            p.requires_grad = True\n    \n    def forward(self, inp, targets):\n        x_emb = self.C[inp]  # Embedding lookup for input data\n        x_emb += position\n\n        # New code\n        # -----------------------------------------------------\n        k = x_emb @ self.key\n        q = x_emb @ self.query\n\n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        \n        tril = torch.tril(torch.ones(block_size, block_size))\n        wei = wei.masked_fill(tril==0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        v = x_emb @ self.value\n        out = wei @ v\n        \n        out = out @ self.proj\n        \n        # Feed-Forward\n        out = self.ffwd(out)\n        # ----------------------------------------------------\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters:\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 11168\n\n\n\noptimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.9821, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n4. Layer Norm\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.inp = nn.Linear(n_emb, 3 * n_emb)\n        self.fc1 = nn.Linear(3 * n_emb, n_emb)\n        \n    def forward(self, x):\n        x = self.inp(x)\n        x = F.relu(x)\n        out = self.fc1(x) \n        return out\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Single-Head Attention Layer\n        self.head = Head(head_size)\n        self.proj = nn.Linear(head_size, n_emb)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n        # New code\n        # ------------------------------------\n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        self.ln3 = nn.LayerNorm(n_emb)\n        # ------------------------------------\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n\n        # New code\n        # -----------------------------------------------------\n        # Single-Head Attention\n        out = self.head(self.ln1(x_emb))\n        out = self.proj(out)\n        \n        # Feed-Forward\n        out = self.ffwd(self.ln2(out))\n\n        # Final layer normalization\n        out = self.ln3(out)\n        # ----------------------------------------------------\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2237, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n5. Multi-head Attention\n\n# Get a batch of data\nxb, yb = get_batch(data, batch_size, block_size)\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n# Embed input data\nx_emb = C[xb]\nx_emb += position\n\n# New code\n# ----------------------------------------------------------------------\n# Initialize key and query matrices\nkey_list = [torch.randn(n_emb, head_size // n_head) * \\\n            n_emb ** -0.5 for headi in range(n_head)]\nquery_list = [torch.randn(n_emb, head_size // n_head) * \\\n              n_emb ** -0.5 for headi in range(n_head)]\n\n# Calculate the key and query values\nk = torch.stack([x_emb @ key for key in key_list], dim = -1).view\\\n                                    (xb.shape[0], block_size, -1)\nq = torch.stack([x_emb @ query for query in query_list], dim = -1).view\\\n                                    (xb.shape[0], block_size, -1)\n# ----------------------------------------------------------------------\n\nprint(k.shape)\nprint(q.shape)\n\n# Compute the dot product between queries and keys\nwei = q @ k.transpose(-2, -1)\nprint(wei.shape)\n\ntril = torch.tril(torch.ones(block_size, block_size))\nwei = wei.masked_fill(tril==0, float('-inf')) \nwei = F.softmax(wei, dim=-1)\n\nprint(wei[0])\n\ntorch.Size([64, 8, 20])\ntorch.Size([64, 8, 20])\ntorch.Size([64, 8, 8])\ntensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [2.7624e-02, 9.7238e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [6.3034e-07, 9.9980e-01, 2.0172e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [8.8760e-07, 9.9450e-01, 5.5003e-03, 3.7328e-10, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [6.4108e-01, 2.2718e-05, 4.7551e-02, 3.0472e-01, 6.6321e-03, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [3.5899e-09, 2.0306e-06, 1.2872e-01, 1.9127e-11, 9.4477e-13, 8.7128e-01,\n         0.0000e+00, 0.0000e+00],\n        [3.4409e-09, 2.2824e-02, 8.0748e-01, 4.0223e-02, 5.4004e-03, 1.2398e-01,\n         9.4144e-05, 0.0000e+00],\n        [1.4672e-07, 9.9970e-01, 3.0362e-04, 2.6681e-13, 2.8710e-12, 1.0302e-06,\n         2.3637e-08, 2.4413e-09]])\n\n\n\n# New code\n# ----------------------------------------------------------------------\nvalue_list = [torch.randn(n_emb, head_size // n_head) * \\\n              n_emb ** -0.5 for headi in range(n_head)]\n\nv = torch.stack([x_emb @ value for value in value_list], dim = -1).view\\\n                                        (xb.shape[0], block_size, -1)\n# ----------------------------------------------------------------------\n\nprint(v.shape)\nout = wei @ v\nout.shape\n\ntorch.Size([64, 8, 20])\n\n\ntorch.Size([64, 8, 20])\n\n\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\n# New code\n# -----------------------------------------------------------------\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size // n_head\n        \n        self.heads = nn.ModuleList([Head(self.head_size) \\\n                                    for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n# -----------------------------------------------------------------\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # New code\n        # -----------------------------------------------------\n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_head, head_size)\n        # -----------------------------------------------------\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n        \n        # New code\n        # -----------------------------------------------------\n        # Multi-Head Attention\n        out = self.mul_head(self.ln1(x_emb))\n        # ----------------------------------------------------\n        \n        # Feed-Forward\n        out = self.ffwd(self.ln2(out))\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n6. Residual\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size // n_head\n        \n        self.heads = nn.ModuleList([Head(self.head_size) \\\n                                    for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_head, head_size)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n        \n        # New code\n        # ---------------------------------------------------------\n        # Multi-Head Attention\n        out = x_emb + self.mul_head(self.ln1(x_emb))\n        \n        # Feed-Forward\n        out = out + self.ffwd(self.ln2(out))\n        # ---------------------------------------------------------\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\nmodel = BigramLanguageModel()\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2127, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "knowledge.html#clean-code",
    "href": "knowledge.html#clean-code",
    "title": "Attention Architecture",
    "section": "Clean code",
    "text": "Clean code\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, n_emb, block_size, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        \n        # Masking to make sure the network can't attend to the future positions\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        \n        # Applying softmax to get the attention probabilities\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_emb, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        \n        self.heads = nn.ModuleList([Head(n_emb, block_size, head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, n_emb, n_head):\n        super().__init__()\n        \n        head_size = n_emb // n_head\n        \n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_emb, n_head, head_size)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n\n    def forward(self, x):\n        # Multi-Head Attention Block\n        x = x + self.mul_head(self.ln1(x))\n        \n        # Feed-Forward Block\n        x = x + self.ffwd(self.ln2(x))\n        \n        return x\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers):\n        super().__init__()\n        self.n_layers = n_layers\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_emb, n_head) for _ in range(n_layers)])\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.shape[1]))\n        x_emb = token_emb + position_emb\n        \n        # Transformer blocks\n        out = self.blocks(x_emb)\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n\nn_layers = 1\nmodel = BigramLanguageModel(vocab_size, n_emb, block_size, n_head, n_layers)\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 15073\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2123, grad_fn=&lt;NllLossBackward0&gt;)"
  }
]