{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nano GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p47XXnu5-Seo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kx7wVb4KXZAL",
    "outputId": "23b53282-62bf-49fa-d929-ff7726954aff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qmCdzaaI_AAx"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 256\n",
    "n_head = 6\n",
    "n_emb = 384\n",
    "batch_size = 64\n",
    "lr = 6e-4\n",
    "n_epochs = 5000\n",
    "n_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RmxeZmD--qfQ"
   },
   "outputs": [],
   "source": [
    "with open('/content/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# Embedding\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character Encoding\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l:  ''.join([itos[i] for i in l])\n",
    "\n",
    "# Convert all text data to integers\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eW3vyes-6T9",
    "outputId": "d787277a-4d25-413f-8557-5f9eaaca1ad2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 8]), torch.Size([512]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size, block_size, device):\n",
    "    # Generate random indices within the valid range\n",
    "    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n",
    "\n",
    "    # Extract blocks of data using the generated indices\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Extract corresponding target blocks\n",
    "    # Note that yb is reshaped to a 1D tensor\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n",
    "\n",
    "    return xb.to(device), yb.to(device)\n",
    "\n",
    "xb, yb = get_batch(data, 64, 8, device)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r_egaLFP-AB2"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_emb, 3 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3 * n_emb, n_emb)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SrczCMVo-IIL"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_emb, block_size, head_size, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear transformations for key, query, and value\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for masking\n",
    "        self.tril = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        # Linear transformations for key and query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "\n",
    "        # Masking to make sure the network can't attend to the future positions\n",
    "        wei.masked_fill_(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Applying softmax to get the attention probabilities\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Linear transformation for value and computing the output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mln8PYnr-IwA"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_emb, n_head, head_size, device):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(n_emb, block_size, head_size, device) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply all attention heads in parallel\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "        # Project the concatenated results\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EsGTKtBO-Lbs"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head, device):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_emb // n_head\n",
    "\n",
    "        # Multi-Head Attention Layer\n",
    "        self.mul_head = MultiHeadAttention(n_emb, n_head, head_size, device)\n",
    "\n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "\n",
    "        # Layer Normalization Layers\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-Head Attention Block\n",
    "        x = x + self.mul_head(self.ln1(x))\n",
    "\n",
    "        # Feed-Forward Block\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0Bhj8WX2-NOi"
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers, device):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head, device) for _ in range(n_layers)])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, inp, targets = None):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.shape[1], device = device))\n",
    "        x_emb = token_emb + position_emb\n",
    "\n",
    "        # Transformer blocks\n",
    "        out = self.blocks(x_emb)\n",
    "\n",
    "        # Final layer normalization\n",
    "        out = self.ln_f(out)\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets == None:\n",
    "          loss = None\n",
    "        else:\n",
    "          # Reshape logits for the cross-entropy loss\n",
    "          logits = logits.view(-1, logits.shape[-1])\n",
    "          # Compute the cross-entropy loss\n",
    "          loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGHBFC6o-Nsy",
    "outputId": "ba52b805-49cd-48f4-db14-9b8453811919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 14817\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, 32, 8, 5, 1, device)\n",
    "model.to(device)\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters():\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klwkq_hn-fvU",
    "outputId": "0a52e8f0-23ed-4829-9adf-b35f6e2c6724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2507, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "for i in range(100):\n",
    "    xb, yb = get_batch(data, 64, 8, device)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "no5985HkBkp9",
    "outputId": "8b8330f0-fdb4-4a55-99e9-bd5a67005456"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros(1, 1, dtype = torch.long, device = device)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FLXksQgDfrN",
    "outputId": "4a6ad172-876c-4ae3-960c-218aa7d99a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.4532,  2.6264, -0.8126, -1.4322, -2.1681, -0.8644,  0.4027,\n",
       "           -1.0778, -1.0430, -1.5876, -0.7976, -0.3464, -1.5724, -0.4819,\n",
       "           -0.4304, -1.0133, -1.5605, -0.8962, -1.4719, -1.2092, -0.6767,\n",
       "           -0.7557, -2.4317, -0.9886, -1.3998, -0.8497, -1.0715, -0.5712,\n",
       "           -1.4491, -1.8244, -1.1285, -0.6197, -0.4073, -1.3509, -1.4581,\n",
       "           -1.0151, -0.5341, -1.5363, -2.2366,  1.2976,  0.1806,  0.0059,\n",
       "            0.4877,  1.7831,  0.3746, -0.1175,  0.9814,  1.0195, -2.1851,\n",
       "           -0.5097,  0.4699,  0.6254,  0.6517,  1.4879, -0.5892, -1.6162,\n",
       "            0.4139,  0.7043,  1.0963,  0.6129, -1.0932,  0.2736, -1.5890,\n",
       "            0.4178, -1.9275]]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " torch.Size([1, 1, 65]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, _ = model(context)\n",
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoL7jzICHX1l",
    "outputId": "515b4059-a11f-40ad-c4d9-ab75526f1786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits[:, -1, :]\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53oiKTUDD5VO",
    "outputId": "f298378f-2f33-413b-cd65-6634a4924504"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0556, 0.1796, 0.0058, 0.0031, 0.0015, 0.0055, 0.0194, 0.0044, 0.0046,\n",
       "         0.0027, 0.0059, 0.0092, 0.0027, 0.0080, 0.0084, 0.0047, 0.0027, 0.0053,\n",
       "         0.0030, 0.0039, 0.0066, 0.0061, 0.0011, 0.0048, 0.0032, 0.0056, 0.0045,\n",
       "         0.0073, 0.0031, 0.0021, 0.0042, 0.0070, 0.0086, 0.0034, 0.0030, 0.0047,\n",
       "         0.0076, 0.0028, 0.0014, 0.0476, 0.0156, 0.0131, 0.0212, 0.0773, 0.0189,\n",
       "         0.0116, 0.0347, 0.0360, 0.0015, 0.0078, 0.0208, 0.0243, 0.0249, 0.0575,\n",
       "         0.0072, 0.0026, 0.0197, 0.0263, 0.0389, 0.0240, 0.0044, 0.0171, 0.0027,\n",
       "         0.0197, 0.0019]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b2K02k0GYii",
    "outputId": "9226484e-9948-4eb6-d652-949d40526872"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[43]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5EcGNGSHMXs",
    "outputId": "bde7d508-b48c-424b-da5c-b1ff93ed4889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 43]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.cat((context, idx_next), dim=1) # (B, T+1)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6zPqvRRHqXS",
    "outputId": "6b0baf0c-b69c-494a-c739-c410404851fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1, 42,  1, 54, 46, 56, 41, 46, 63, 59]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros(1, 1, dtype = torch.long, device = device)\n",
    "for i in range(10):\n",
    "  idx_cond = context[:, -8:]\n",
    "  logits, _ = model(idx_cond.to(device))\n",
    "  logits = logits[:, -1, :]\n",
    "  probs = F.softmax(logits, dim=-1)\n",
    "  idx_next = torch.multinomial(probs.to(device), num_samples=1)\n",
    "  context = torch.cat((context, idx_next), dim=1)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_uwTdM0NHDf",
    "outputId": "cda84bc3-9262-4f12-82b1-0caf8eb574e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " d phrchyu\n"
     ]
    }
   ],
   "source": [
    "print(decode(context[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mP090X1YNV4h"
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers, device):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head, device) for _ in range(n_layers)])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, inp, targets = None):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.shape[1], device = device))\n",
    "        x_emb = token_emb + position_emb\n",
    "\n",
    "        # Transformer blocks\n",
    "        out = self.blocks(x_emb)\n",
    "\n",
    "        # Final layer normalization\n",
    "        out = self.ln_f(out)\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets == None:\n",
    "          loss = None\n",
    "        else:\n",
    "          # Reshape logits for the cross-entropy loss\n",
    "          logits = logits.view(-1, logits.shape[-1])\n",
    "          # Compute the cross-entropy loss\n",
    "          loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjHe2jU7PxUz",
    "outputId": "fcc3c2eb-ef8f-41da-9c6a-f5416c78d269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lAiangego qt s rhrditts yGED oEM thloXtoYbe&tnsaweco&ilx e,zv: ostnGho  tris'Tste elbcolrPguQ  K&d b\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, 32, 8, 5, 1, device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "for i in range(100):\n",
    "    xb, yb = get_batch(data, 64, 8, device)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "context = torch.zeros(1, 1, dtype = torch.long, device = device)\n",
    "context = model.generate(context, 100)\n",
    "print(decode(context[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHyRHQkzQKW8",
    "outputId": "aaf5592d-6736-4c28-b825-4979777ede04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training time: 12 min 32.97s\n",
      "Loss: 1.3645884990692139\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Get the start time\n",
    "start_time = time.time()\n",
    "\n",
    "model = BigramLanguageModel(vocab_size, batch_size, block_size, n_head, n_layers, device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size, device)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Get the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert to minutes and seconds\n",
    "execution_time_minutes = int(execution_time // 60)\n",
    "execution_time_seconds = round(execution_time % 60, 2)\n",
    "\n",
    "print(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KgQxAhrTSxV",
    "outputId": "d0c920b6-0301-4284-e0d1-7b4a8a65c032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BINCHURD:\n",
      "How would prosom.\n",
      "\n",
      "CORIOLANUS:\n",
      "Withre, wish'd Tyrrel! What! sovereign will their wound\n",
      "The leason, mean must bone of any wipsoy,\n",
      "Sprenators are the unclaol you both:\n",
      "I did! Lord not calmise she himself,\n",
      "So not time and sto let your griop,\n",
      "And not themself he; but fond Edward's;\n",
      "I dothous of my this nection Ledia cravol'd thousand:\n",
      "Or thou murderst myself by moron.\n",
      "Here good no fheash which, for his means: not\n",
      "That may massy pass himself.\n",
      "\n",
      "Secondamen:\n",
      "When follows of the ragin of at Juldeen.\n",
      "\n",
      "RICHMOND:\n",
      "Artimence\n",
      "O had him; he down, if you'rt,--that'd hearts; where's young,\n",
      "He is on, the my life.\n",
      "\n",
      "Bids Senator:\n",
      "Yet be do no tongue, which you manuch London.\n",
      "If you, myself, I'll swear'd life de'er anger,\n",
      "for you should leave you, beholds on dawn;\n",
      "If you my widest throat dost your did.\n",
      "\n",
      "SICINIUS:\n",
      "Durstily lamish their fly pirdon, vourtur you\n",
      "Or herume, your upon to you with's.\n",
      "\n",
      "JOMN.\n",
      "\n",
      "CALISA:\n",
      "And mayling her And God, and live.\n",
      "\n",
      "Boold Marcius of Glord!\n",
      "\n",
      "TRANIO:\n",
      "I have home; and, I neven been her with easl.\n",
      "\n",
      "Nuse:\n",
      "No, if you comble know the were moonst thou state,\n",
      "Let in Rutless, I will for an my compage,\n",
      "Were foom out now sault.\n",
      "\n",
      "LUCIO:\n",
      "Alisa! Ha, vife, somemans, only thy earth,\n",
      "This rancions Isap live; on her Volsce,\n",
      "A famought menalsy all you thine men his,\n",
      "But now the could give of Kentom where's thou none:\n",
      "Padam, help My hath should death in away't.\n",
      "\n",
      "DUKE OF YORK:\n",
      "BaptatiOSons, thou yourself: how tidstruled are\n",
      "The beet that coasks that him: I will flesh,\n",
      "Still thou warried, I am benefit's band,\n",
      "Why having end he's tongue a shown. My lord;\n",
      "Ay, to veing, he must will nicemploidy:\n",
      "Why, brother? stays the your nust chiefs like half?\n",
      "for the lord, gay you greating to its is mine;\n",
      "How sorrow to-taffe, sir blood him.\n",
      "\n",
      "GANE:\n",
      "No, whilst thou vinechably deep mean\n",
      "Should he denilsh. I humbout him, poort them well.\n",
      "Yes no loyalong a mother, doth daughter-scrillence\n",
      "Run thou menfords: my doom Edward's pieces\n",
      "with neverfules are, and like no.\n",
      "I thene--is I, away good bittle him time\n",
      "Approuched not the wears earth come his out:\n",
      "I'll peach\n",
      "Which worse this dafby after tearth:\n",
      "And no esport yet through Stiffortly:\n",
      "Found by mostion, and seems yearsen,\n",
      "To him blood betwixt with right forbeign.\n",
      "Avoiding Gentleman,\n",
      "Upon of you suith'd from.\n",
      "Take I not your maje, the thought sleep from Brancish her.\n",
      "\n",
      "VALENIO:\n",
      "And not proominal; flear, I'll your may.\n",
      "\n",
      "BENVOLIO:\n",
      "How moust that donature heart, by strept's not\n",
      "seek the one power ashe crevent out.\n",
      "\n",
      "Second Citizen:\n",
      "That thy could lords, O meet them name,\n",
      "The show will powery shall thee bashedly wretch,\n",
      "That here a mile do.\n",
      "\n",
      "CAPULET:\n",
      "Go anst madam, Sir, that you comes wont pilt.\n",
      "When the poor acheect in the charm.\n",
      "\n",
      "ESCALUS:\n",
      "The world here quarrelly, my father's well.\n",
      "\n",
      "PETRUCHIO:\n",
      "With so. Why he had Six Aumey?\n",
      "\n",
      "LOMETEL:\n",
      "Nay, so marry neather me whom hie be fearful so.\n",
      "\n",
      "GLOUCESTER:\n",
      "Let shall a splew'd util raise Mauch done.\n",
      "King Vistiber in false him improcy!\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Good Retranged, our my long; as prison,--\n",
      "\n",
      "LADY ANNE:\n",
      "I must don, madne when thou stay the temw Edward?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Know night-scaley's make addening,--Those she spiliented.\n",
      "The Chatard host known thy bound came;\n",
      "Bith thou sand now slain are, leave not the\n",
      "Time friar a caulamony strongerous; when my peace.\n",
      "What? have are it my vial man\n",
      "Deceince scopul'd but not this do\n",
      "Unheav'd look brother: we heaven! Guession.\n",
      "\n",
      "Nurse:\n",
      "I need.\n",
      "\n",
      "ELWARD:\n",
      "Ay, say, if impatance this childrent him;\n",
      "As he wrong a made poising modes and most\n",
      "to fact not pleasure of the dispak, if all,\n",
      "Your hauntily come; my Lnord As stay the is,\n",
      "That by jollior at bearn, -my wife;\n",
      "Thou shouldst corn in he'r beaut, the word.\n",
      "\n",
      "DUKE OF YORK:\n",
      "How are now, there, them, her: And, forbund then\n",
      "Obacter's dattly nothing restors: he daughter.\n",
      "\n",
      "CORIOLANUS:\n",
      "Wile she sister, as wish birmly down.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "That he good much, thou worst may thou accuse,\n",
      "And how herself cares change art to\n",
      "might now! Captaius\n",
      "\n",
      "GLOUCESTER:\n",
      "I would a good ap, I have redly Georg the so beggar,\n",
      "Thou knew'st not to have much well to noble;\n",
      "And auntiver PomuLe Edward's not me-both;\n",
      "I'll say repingry,\n",
      "Tybalt, my lordnex Duked.\n",
      "\n",
      "PETRUCHIO:\n",
      "In have won his tormish'd provope, an Canstator!\n",
      "Therefort, downs by Grumishanes, west I execute\n",
      "That anscher-bone fearful encouse\n",
      "'Than so with your sixtemneds. Who silves,\n",
      "The grien every had left of his affeir,\n",
      "Cousin, no mishact I tate a sail wint.\n",
      "I brave whom sword thou had thou sea\n",
      "To wash on the murder'st rest of andswer.\n",
      "\n",
      "ARIEL:\n",
      "Therefore, if left your unded.\n",
      "\n",
      "JULIET:\n",
      "Laster, in young benotators of the old:\n",
      "Aid by my heart grace wholow him excute's\n",
      "As idlections by to our lifess constrial\n",
      "Of theirly prepose that that early a\n",
      "Foncy's of countrensment to us, but you inclove.\n",
      "An should the world.\n",
      "\n",
      "KING RICHARD III:\n",
      "It prisons feellutes, hath man incladed\n",
      "The tongle, the Romey's liuse his in\n",
      "Aridey of an glife bonef, insweet him men.\n",
      "There hunger spiake tone so hearl--\n",
      "Conductioush. Whose my he tyble heavens and possak.\n",
      "What a wonte\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros(1, 1, dtype = torch.long, device=device)\n",
    "context = model.generate(context, 5000)\n",
    "print(decode(context[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "i5oqekKBliA3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
